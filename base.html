<link href="material-components-web.min.css" rel="stylesheet">
<!-- <script src="https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3"></script> -->
<!-- <script src="material-components-web.min.tsx"></script> -->

<h1 style="text-align: center;">Arabic Hand Sign Gestures Recognition <br> using the Convolutional Neural Network & MediaPipe</h1>
<!--  -->
<section id="demos" class="invisible">
  <h2><br>Arabic Hand Sign Gestures Recognition</h2>
  <p>Use your hand to make signs in front of the camera to get signs classification. </br>Click <b>enable webcam</b> below and grant access to the webcam if prompted.</p>

  <div id="liveView" class="videoView">
    <div class="centered-container">
      <button id="webcamButton" class="mdc-button mdc-button--raised">
        <span class="mdc-button__ripple"></span>
        <span class="mdc-button__label">ENABLE WEBCAM</span>
      </button>
    </div>
    <div style="position: relative;">
      <video id="webcam" autoplay playsinline></video>
      <canvas class="output_canvas" id="output_canvas" width="1280" height="720" style="position: absolute; left: 0px; top: 0px;"></canvas>
      <p id='gesture_output' class="output"></p>
    </div>
  </div>
</section>

<!-- 
<section id="demos" class="invisible">
  <h2>Demo: Recognize gestures</h2>
  <p><em>Click on an image below</em> to identify the gestures in the image.</p>

  <div class="detectOnClick">
    <img src="https://assets.codepen.io/9177687/idea-gcbe74dc69_1920.jpg" crossorigin="anonymous" title="Click to get recognize!" />
    <p class="classification removed">
  </div>
  <div class="detectOnClick">
    <img src="https://assets.codepen.io/9177687/thumbs-up-ga409ddbd6_1.png" crossorigin="anonymous" title="Click to get recognize!" />
    <p class="classification removed">
  </div>
</section> -->

<script type="module">
  import {
  GestureRecognizer,
  FilesetResolver,
  DrawingUtils
  } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3";

  const demosSection = document.getElementById("demos");
  let gestureRecognizer= GestureRecognizer;
  let runningMode = "IMAGE";
  let enableWebcamButton= HTMLButtonElement;
  let webcamRunning = false;
  const videoHeight = "360px";
  const videoWidth = "480px";

  // Before we can use HandLandmarker class we must wait for it to finish
  // loading. Machine Learning models can be large and take a moment to
  // get everything needed to run.
  const createGestureRecognizer = async () => {
    const vision = await FilesetResolver.forVisionTasks(
      "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3/wasm"
    );
    gestureRecognizer = await GestureRecognizer.createFromOptions(vision, {
      baseOptions: {
        modelAssetPath:
          "gesture_recognizer.task",
        delegate: "GPU"
      },
      runningMode: runningMode
    });
    demosSection.classList.remove("invisible");
  };
  createGestureRecognizer();


  /********************************************************************
  // Continuously grab image from webcam stream and detect it.
  ********************************************************************/

  const video = document.getElementById("webcam");
  const canvasElement = document.getElementById("output_canvas");
  const canvasCtx = canvasElement.getContext("2d");
  const gestureOutput = document.getElementById("gesture_output");

  // Check if webcam access is supported.
  function hasGetUserMedia() {
    return !!(navigator.mediaDevices && navigator.mediaDevices.getUserMedia);
  }

  // If webcam supported, add event listener to button for when user
  // wants to activate it.
  if (hasGetUserMedia()) {
    enableWebcamButton = document.getElementById("webcamButton");
    enableWebcamButton.addEventListener("click", enableCam);
  } else {
    console.warn("getUserMedia() is not supported by your browser");
  }

  // Enable the live webcam view and start detection.
  function enableCam(event) {
    if (!gestureRecognizer) {
      alert("Please wait for gestureRecognizer to load");
      return;
    }

    if (webcamRunning === true) {
      webcamRunning = false;
      enableWebcamButton.innerText = "ENABLE PREDICTIONS";
    } else {
      webcamRunning = true;
      enableWebcamButton.innerText = "DISABLE PREDICTIONS";
    }

    // getUsermedia parameters.
    const constraints = {
      video: true
    };

    // Activate the webcam stream.
    navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {
      video.srcObject = stream;
      video.addEventListener("loadeddata", predictWebcam);
    });
  }

  let lastVideoTime = -1;
  let results = undefined;

  let previousLetter = '';
  let sentence = ''; 

  async function predictWebcam() {
    const webcamElement = document.getElementById("webcam");
    // Now let's start detecting the stream.
    if (runningMode === "IMAGE") {
      runningMode = "VIDEO";
      await gestureRecognizer.setOptions({ runningMode: "VIDEO" });
    }
    let nowInMs = Date.now();
    if (video.currentTime !== lastVideoTime) {
      lastVideoTime = video.currentTime;
      results = gestureRecognizer.recognizeForVideo(video, nowInMs);
    }

    canvasCtx.save();
    canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
    const drawingUtils = new DrawingUtils(canvasCtx);

    canvasElement.style.height = videoHeight;
    webcamElement.style.height = videoHeight;
    canvasElement.style.width = videoWidth;
    webcamElement.style.width = videoWidth;

    if (results.landmarks) {
      for (const landmarks of results.landmarks) {
        drawingUtils.drawConnectors(
          landmarks,
          GestureRecognizer.HAND_CONNECTIONS,
          {
            color: "#00FF00",
            lineWidth: 5
          }
        );
        drawingUtils.drawLandmarks(landmarks, {
          color: "#FF0000",
          lineWidth: 2
        });
      }
    }
    const arabicMapping = {
      'Seen': 'س',
      'Ain': 'ع',
      'Al': 'ال',
      'Waw': 'و',
      'Teh': 'ت',
      'Zain': 'ز',
      'Dad': 'ض',
      'Heh': 'ه',
      'Lam': 'ل',
      'Meem': 'م',
      'Theh': 'ظ',
      'Laa': 'لا',
      'Jeem': 'ج',
      'Reh': 'ر',
      'Beh': 'ب',
      'Kaf': 'ك',
      'Feh': 'ف',
      'Alef': 'ا',
      'Tah': 'ط',
      'Thal': 'ذ',
      'Sheen': 'ش',
      'Khah': 'خ',
      'Hah': 'ح',
      'Ghain': 'غ',
      'Noon': 'ن',
      'Sad': 'ص',
      'Teh_Marbuta': 'ة',
      'Qaf': 'ق',
      'Dal': 'د',
      'Zah': 'ظ',
      'Yeh': 'ي'
    };



    canvasCtx.restore();
    if (results.gestures.length > 0) {
      gestureOutput.style.display = "block";
      gestureOutput.style.width = videoWidth;

      let categoryName = arabicMapping[results.gestures[0][0].categoryName];

      if (categoryName === undefined) {
        categoryName = "none";
      }
      const categoryScore = parseFloat(
        results.gestures[0][0].score * 100
      ).toFixed(2);
      const handedness = results.handednesses[0][0].displayName;

      if (categoryScore > 82) {
        const currentLetter = arabicMapping[results.gestures[0][0].categoryName] || ' ';
        if (currentLetter !== 'none' && currentLetter !== previousLetter) {
          sentence += currentLetter;
          previousLetter = currentLetter;
        }
      }      

      gestureOutput.innerText = `GestureRecognizer: ${categoryName}\n Sentence: ${sentence}\n Confidence: ${categoryScore} %\n Handedness: ${handedness}`;
    } else {
      gestureOutput.style.display = "none";
    }

    // Call this function again to keep predicting when the browser is ready.
    if (webcamRunning === true) {
      window.requestAnimationFrame(predictWebcam);
    }
  }

</script>